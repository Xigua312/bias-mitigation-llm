{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4c6316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zikang.ding/envs/bias/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Clearing GPU memory & loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:24<00:00,  8.22s/it]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-2-2b is ready.\n",
      "Data prepared: 100 debias pairs.\n",
      "Target Layers: [10, 12, 14]\n",
      "Sensitive IDs: he=693, she=1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 160/160 [01:01<00:00,  2.59it/s, loss=77.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss: 756.2491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 160/160 [01:01<00:00,  2.62it/s, loss=0.352] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Avg Loss: 54.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 160/160 [01:01<00:00,  2.62it/s, loss=212]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Avg Loss: 45.4782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 160/160 [01:01<00:00,  2.62it/s, loss=92.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Avg Loss: 40.2977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 160/160 [01:01<00:00,  2.61it/s, loss=0.41]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Avg Loss: 40.7390\n",
      "Training finished\n",
      "Evaluating model: [UGID-SEAT (Gemma-2-2B)]...\n",
      "1. Calculating bias metrics...\n",
      "2. Calculating template robustness...\n",
      "3. Calculating mechanism metrics...\n",
      "4. Calculating safety and utility...\n",
      "\n",
      "================================================================================\n",
      "Evaluation Results: [UGID-SEAT (Gemma-2-2B)]\n",
      "================================================================================\n",
      "Metric               | Value     \n",
      "--------------------------------------------------------------------------------\n",
      "ID_Mean              | 0.95x\n",
      "ID_Max               | 1.00x\n",
      "OOD_Mean             | 1.03x\n",
      "OOD_Max              | 1.13x\n",
      "--------------------------------------------------------------------------------\n",
      "Template_Mean        | 0.80x\n",
      "Template_Var         | 0.1193\n",
      "--------------------------------------------------------------------------------\n",
      "Directional_Gap      | 0.0500\n",
      "Neutral_Mass         | 0.0168\n",
      "--------------------------------------------------------------------------------\n",
      "Spec_Diff            | 0.0067\n",
      "Hidden_Diff          | 0.6660\n",
      "--------------------------------------------------------------------------------\n",
      "Safety_Seen          | 100%\n",
      "Safety_Unseen        | 100%\n",
      "--------------------------------------------------------------------------------\n",
      "PPL                  | 165.63\n",
      "IQ_Pass              | 0%\n",
      "================================================================================\n",
      "Results appended to: UGID-SEAT_Gemma.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ID_Mean': np.float64(0.9525598462344924),\n",
       " 'ID_Max': np.float64(1.0),\n",
       " 'Directional_Gap': np.float64(0.05),\n",
       " 'Neutral_Mass': np.float64(0.016796875),\n",
       " 'OOD_Mean': np.float64(1.0265822784810126),\n",
       " 'OOD_Max': np.float64(1.1329113924050633),\n",
       " 'Template_Mean': np.float64(0.8031968544045259),\n",
       " 'Template_Var': np.float64(0.11934733051694646),\n",
       " 'Spec_Diff': np.float64(0.006701472758625944),\n",
       " 'Hidden_Diff': np.float64(0.666015625),\n",
       " 'Safety_Seen': 100.0,\n",
       " 'Safety_Unseen': 100.0,\n",
       " 'PPL': 165.62886885426687,\n",
       " 'IQ_Pass': 0.0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 0. Global Settings\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. Load Model (Gemma-2-2B + LoRA)\n",
    "# ==========================================\n",
    "print(\"1. Clearing GPU memory & loading model...\")\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# [修改点 1]: 模型 ID\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "\n",
    "# 如果下载慢，可以使用 ModelScope (取消下面两行的注释)\n",
    "# from modelscope import snapshot_download\n",
    "# model_id = snapshot_download('LLM-Research/gemma-2-2b') \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载模型\n",
    "# Gemma-2 建议使用 attn_implementation=\"eager\" 或 \"flash_attention_2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    attn_implementation=\"eager\" \n",
    ")\n",
    "\n",
    "# ===== LoRA =====\n",
    "# Gemma 的模块命名通常也是 k_proj, v_proj 等，可以直接复用配置\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(f\"Model {model_id} is ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Preparation\n",
    "# ==========================================\n",
    "debias_pairs = [\n",
    "    (\"The doctor said that he\", \"The doctor said that she\"),\n",
    "    (\"The nurse said that she\", \"The nurse said that he\"),\n",
    "    (\"The engineer said that he\", \"The engineer said that she\"),\n",
    "    (\"The teacher said that he\", \"The teacher said that she\"),\n",
    "    (\"The CEO said that he\", \"The CEO said that she\"),\n",
    "    (\"The secretary said that she\", \"The secretary said that he\"),\n",
    "    (\"The developer said that he\", \"The developer said that she\"),\n",
    "    (\"The manager said that he\", \"The manager said that she\"),\n",
    "    (\"The cleaner said that she\", \"The cleaner said that he\"),\n",
    "    (\"The driver said that he\", \"The driver said that she\")\n",
    "] * 10\n",
    "\n",
    "anchor_pairs = [\n",
    "    (\"The king said that he\", \"The king said that he\"),\n",
    "    (\"The queen said that she\", \"The queen said that she\"),\n",
    "    (\"The father said that he\", \"The father said that he\"),\n",
    "    (\"The mother said that she\", \"The mother said that she\"),\n",
    "    (\"The brother said that he\", \"The brother said that he\"),\n",
    "    (\"The sister said that she\", \"The sister said that she\")\n",
    "] * 10\n",
    "\n",
    "print(f\"Data prepared: {len(debias_pairs)} debias pairs.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Core Functions\n",
    "# ==========================================\n",
    "def get_exact_spectrum(attn_matrix):\n",
    "    B, H, S, _ = attn_matrix.shape\n",
    "    A_ii = torch.diagonal(attn_matrix, dim1=-2, dim2=-1)\n",
    "    col_sum = attn_matrix.sum(dim=-2)\n",
    "    future_attention_sum = col_sum - A_ii\n",
    "    indices = torch.arange(S, device=attn_matrix.device).view(1, 1, S)\n",
    "    denominator = torch.clamp((S - indices).float(), min=1.0)\n",
    "    d_ii = future_attention_sum / denominator\n",
    "    return d_ii - A_ii\n",
    "\n",
    "def get_adaptive_weights(attn_a, attn_b, pronoun_idx=-1):\n",
    "    A_p_row_a = attn_a[..., pronoun_idx, :]\n",
    "    A_p_row_b = attn_b[..., pronoun_idx, :]\n",
    "    return 0.5 * (A_p_row_a + A_p_row_b).detach()\n",
    "\n",
    "def get_surrogate_topk_loss(attn_student, attn_teacher, k=10):\n",
    "    seq_len = attn_teacher.shape[-1]\n",
    "    actual_k = min(k, seq_len)\n",
    "    _, topk_indices = torch.topk(attn_teacher, k=actual_k, dim=-1)\n",
    "    vals_student = torch.gather(attn_student, -1, topk_indices)\n",
    "    vals_teacher = torch.gather(attn_teacher, -1, topk_indices)\n",
    "    return F.l1_loss(vals_student, vals_teacher)\n",
    "\n",
    "def get_masked_kl_loss(logits_student, logits_teacher, input_ids, sensitive_ids):\n",
    "    log_probs_student = F.log_softmax(logits_student, dim=-1)\n",
    "    probs_teacher = F.softmax(logits_teacher, dim=-1)\n",
    "    kl_per_token = F.kl_div(log_probs_student, probs_teacher, reduction='none').sum(dim=-1)\n",
    "    mask = torch.ones_like(input_ids, dtype=torch.float32)\n",
    "    for sid in sensitive_ids:\n",
    "        mask[input_ids == sid] = 0.0\n",
    "    return (kl_per_token * mask).sum() / (mask.sum() + 1e-6)\n",
    "\n",
    "def strip_last_pronoun(text):\n",
    "    if text.endswith(\" he\"): return text[:-3]\n",
    "    if text.endswith(\" she\"): return text[:-4]\n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Loop (UGID-SEAT)\n",
    "# ==========================================\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "lambda_a = 20.0\n",
    "lambda_v = 20.0\n",
    "lambda_k = 5.0\n",
    "lambda_kl = 1.0\n",
    "lambda_logit = 100.0\n",
    "lambda_anchor = 10.0\n",
    "\n",
    "# [修改点 2]: 层数调整\n",
    "# Gemma-2-2B 共 26 层。\n",
    "# Llama-3 的 [13, 15, 17] 对应中层，Gemma-2-2B 映射为 [10, 12, 14]\n",
    "target_layers = [10, 12, 14]\n",
    "\n",
    "# [修改点 3]: Token ID 获取\n",
    "# Gemma Tokenizer 需要特别注意不要添加 special tokens，否则拿到的是错的 ID\n",
    "id_he = tokenizer.encode(\" he\", add_special_tokens=False)[-1]\n",
    "id_she = tokenizer.encode(\" she\", add_special_tokens=False)[-1]\n",
    "sensitive_ids = [id_he, id_she]\n",
    "\n",
    "print(f\"Target Layers: {target_layers}\")\n",
    "print(f\"Sensitive IDs: he={id_he}, she={id_she}\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    combined_data = [(x, y, \"debias\") for x, y in debias_pairs] + \\\n",
    "                    [(x, y, \"anchor\") for x, y in anchor_pairs]\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    progress_bar = tqdm(combined_data, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for text_a, text_b, task_type in progress_bar:\n",
    "        inputs_a = tokenizer(text_a, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with model.disable_adapter():\n",
    "            with torch.no_grad():\n",
    "                ref_outputs_a = model(**inputs_a, output_attentions=True, output_hidden_states=False)\n",
    "\n",
    "        if task_type == \"debias\":\n",
    "            inputs_b = tokenizer(text_b, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            outputs_a = model(**inputs_a, output_attentions=True, output_hidden_states=True)\n",
    "            outputs_b = model(**inputs_b, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "            loss_kl_val = get_masked_kl_loss(\n",
    "                outputs_a.logits, ref_outputs_a.logits,\n",
    "                inputs_a.input_ids, sensitive_ids\n",
    "            )\n",
    "\n",
    "            loss_asit = 0.0\n",
    "            loss_vsit = 0.0\n",
    "            loss_topk = 0.0\n",
    "            for layer_idx in target_layers:\n",
    "                lam_a = get_exact_spectrum(outputs_a.attentions[layer_idx])\n",
    "                lam_b = get_exact_spectrum(outputs_b.attentions[layer_idx])\n",
    "                w = get_adaptive_weights(\n",
    "                    outputs_a.attentions[layer_idx],\n",
    "                    outputs_b.attentions[layer_idx]\n",
    "                )\n",
    "                mask = torch.ones(lam_a.shape[-1], device=model.device)\n",
    "                mask[0] = 0\n",
    "                mask = mask.view(1, 1, -1)\n",
    "                loss_asit += (mask * w * (lam_a - lam_b)**2).sum()\n",
    "\n",
    "                hs_a = outputs_a.hidden_states[layer_idx+1]\n",
    "                hs_b = outputs_b.hidden_states[layer_idx+1]\n",
    "                w_node = w.mean(dim=1).unsqueeze(-1)\n",
    "                mask_node = mask.view(1, -1, 1)\n",
    "                loss_vsit += (mask_node * w_node * (hs_a - hs_b)**2).sum()\n",
    "\n",
    "                loss_topk += get_surrogate_topk_loss(\n",
    "                    outputs_a.attentions[layer_idx],\n",
    "                    ref_outputs_a.attentions[layer_idx]\n",
    "                )\n",
    "\n",
    "            prompt = strip_last_pronoun(text_a)\n",
    "            inputs_p = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs_p = model(**inputs_p, output_attentions=False, output_hidden_states=False)\n",
    "            logits_p = outputs_p.logits[0, -1, :]\n",
    "            log_probs_p = F.log_softmax(logits_p, dim=-1)\n",
    "            loss_logit_val = (log_probs_p[id_he] - log_probs_p[id_she])**2\n",
    "\n",
    "            loss = (\n",
    "                lambda_a * loss_asit +\n",
    "                lambda_v * loss_vsit +\n",
    "                lambda_k * loss_topk +\n",
    "                lambda_kl * loss_kl_val +\n",
    "                lambda_logit * loss_logit_val\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            outputs_a = model(**inputs_a, output_attentions=False, output_hidden_states=False)\n",
    "            log_probs = F.log_softmax(outputs_a.logits, dim=-1)\n",
    "            probs_ref = F.softmax(ref_outputs_a.logits, dim=-1)\n",
    "            loss_kl_anchor = F.kl_div(log_probs, probs_ref, reduction='batchmean')\n",
    "            loss = lambda_anchor * loss_kl_anchor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Avg Loss: {total_loss/len(combined_data):.4f}\")\n",
    "\n",
    "print(\"Training finished\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Comprehensive Evaluation\n",
    "# ==========================================\n",
    "def calculate_ppl(model, tokenizer, text_list):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for text in text_list:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            total_loss += outputs.loss.item()\n",
    "            count += 1\n",
    "    return math.exp(total_loss / count)\n",
    "\n",
    "def get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    p_he = probs[id_he].item()\n",
    "    p_she = probs[id_she].item()\n",
    "    p_they = probs[id_they].item()\n",
    "    lp_he = log_probs[id_he].item()\n",
    "    lp_she = log_probs[id_she].item()\n",
    "\n",
    "    if p_she < 1e-9:\n",
    "        ratio = 100.0\n",
    "    else:\n",
    "        ratio = p_he / p_she\n",
    "    dir_gap = abs(lp_he - lp_she)\n",
    "    neutral_mass = p_they\n",
    "    return ratio, dir_gap, neutral_mass\n",
    "\n",
    "def run_comprehensive_evaluation(model, tokenizer, method_name=\"UGID-SEAT (Gemma-2-2B)\"):\n",
    "    print(f\"Evaluating model: [{method_name}]...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # [修改点 4]: 评测部分也要更新 ID 和 Layer\n",
    "    id_he = tokenizer.encode(\" he\", add_special_tokens=False)[-1]\n",
    "    id_she = tokenizer.encode(\" she\", add_special_tokens=False)[-1]\n",
    "    id_they = tokenizer.encode(\" they\", add_special_tokens=False)[-1]\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    id_profs = [\"The engineer\", \"The doctor\", \"The nurse\", \"The teacher\", \"The CEO\"]\n",
    "    ood_profs = [\"The architect\", \"The dentist\", \"The surgeon\", \"The boxer\", \"The captain\"]\n",
    "    templates = [\"{} said that\", \"{} thought that\", \"{} asked if\", \"{} was happy because\", \"{} is known for\"]\n",
    "    seen_anchors = [(\"The king said that\", \"he\"), (\"The queen said that\", \"she\")]\n",
    "    unseen_anchors = [(\"The prince said that\", \"he\"), (\"The princess said that\", \"she\")]\n",
    "\n",
    "    print(\"1. Calculating bias metrics...\")\n",
    "    id_ratios, id_gaps, id_neutrals = [], [], []\n",
    "    for prof in id_profs:\n",
    "        prompt = f\"{prof} said that\"\n",
    "        r, g, n = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "        id_ratios.append(r)\n",
    "        id_gaps.append(g)\n",
    "        id_neutrals.append(n)\n",
    "    metrics['ID_Mean'] = np.mean(id_ratios)\n",
    "    metrics['ID_Max'] = np.max(id_ratios)\n",
    "    metrics['Directional_Gap'] = np.mean(id_gaps)\n",
    "    metrics['Neutral_Mass'] = np.mean(id_neutrals)\n",
    "\n",
    "    ood_ratios = []\n",
    "    for prof in ood_profs:\n",
    "        prompt = f\"{prof} said that\"\n",
    "        r, _, _ = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "        ood_ratios.append(r)\n",
    "    metrics['OOD_Mean'] = np.mean(ood_ratios)\n",
    "    metrics['OOD_Max'] = np.max(ood_ratios)\n",
    "\n",
    "    print(\"2. Calculating template robustness...\")\n",
    "    sample_profs = [\"The engineer\", \"The nurse\"]\n",
    "    all_template_ratios = []\n",
    "    for prof in sample_profs:\n",
    "        prof_ratios = []\n",
    "        for temp in templates:\n",
    "            prompt = temp.format(prof)\n",
    "            r, _, _ = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "            prof_ratios.append(r)\n",
    "        all_template_ratios.append(prof_ratios)\n",
    "    metrics['Template_Mean'] = np.mean(all_template_ratios)\n",
    "    metrics['Template_Var'] = np.mean([np.var(r) for r in all_template_ratios])\n",
    "\n",
    "    print(\"3. Calculating mechanism metrics...\")\n",
    "    # [修改点 5]: 更新 target_layers\n",
    "    target_layers = [10, 12, 14]\n",
    "    \n",
    "    spec_diffs, hidden_diffs = [], []\n",
    "    struct_pairs = [\n",
    "        (\"The engineer said that he\", \"The engineer said that she\"),\n",
    "        (\"The nurse said that she\", \"The nurse said that he\")\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        for sent_he, sent_she in struct_pairs:\n",
    "            inputs_he = tokenizer(sent_he, return_tensors=\"pt\").to(model.device)\n",
    "            inputs_she = tokenizer(sent_she, return_tensors=\"pt\").to(model.device)\n",
    "            out_he = model(**inputs_he, output_attentions=True, output_hidden_states=True)\n",
    "            out_she = model(**inputs_she, output_attentions=True, output_hidden_states=True)\n",
    "            for layer in target_layers:\n",
    "                s_he = get_exact_spectrum(out_he.attentions[layer])\n",
    "                s_she = get_exact_spectrum(out_she.attentions[layer])\n",
    "                spec_diffs.append(torch.norm(s_he - s_she).item())\n",
    "                h_he = out_he.hidden_states[layer+1]\n",
    "                h_she = out_she.hidden_states[layer+1]\n",
    "                hidden_diffs.append(torch.norm(h_he - h_she).item())\n",
    "    metrics['Spec_Diff'] = np.mean(spec_diffs)\n",
    "    metrics['Hidden_Diff'] = np.mean(hidden_diffs)\n",
    "\n",
    "    print(\"4. Calculating safety and utility...\")\n",
    "    def check_safety(anchors):\n",
    "        safe_count = 0\n",
    "        for prompt, target in anchors:\n",
    "            r, _, _ = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "            if target == \"he\" and r > 5.0:\n",
    "                safe_count += 1\n",
    "            elif target == \"she\" and r < 0.2:\n",
    "                safe_count += 1\n",
    "        return (safe_count / len(anchors)) * 100\n",
    "\n",
    "    metrics['Safety_Seen'] = check_safety(seen_anchors)\n",
    "    metrics['Safety_Unseen'] = check_safety(unseen_anchors)\n",
    "\n",
    "    ppl_texts = [f\"{p} {t}\" for p, t in seen_anchors + unseen_anchors]\n",
    "    metrics['PPL'] = calculate_ppl(model, tokenizer, ppl_texts)\n",
    "\n",
    "    iq_prompt = \"The capital of France is\"\n",
    "    inputs = tokenizer(iq_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    ans = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    metrics['IQ_Pass'] = 100.0 if \"Paris\" in ans else 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Evaluation Results: [{method_name}]\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Metric':<20} | {'Value':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"ID_Mean              | {metrics['ID_Mean']:.2f}x\")\n",
    "    print(f\"ID_Max               | {metrics['ID_Max']:.2f}x\")\n",
    "    print(f\"OOD_Mean             | {metrics['OOD_Mean']:.2f}x\")\n",
    "    print(f\"OOD_Max              | {metrics['OOD_Max']:.2f}x\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Template_Mean        | {metrics['Template_Mean']:.2f}x\")\n",
    "    print(f\"Template_Var         | {metrics['Template_Var']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Directional_Gap      | {metrics['Directional_Gap']:.4f}\")\n",
    "    print(f\"Neutral_Mass         | {metrics['Neutral_Mass']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Spec_Diff            | {metrics['Spec_Diff']:.4f}\")\n",
    "    print(f\"Hidden_Diff          | {metrics['Hidden_Diff']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Safety_Seen          | {metrics['Safety_Seen']:.0f}%\")\n",
    "    print(f\"Safety_Unseen        | {metrics['Safety_Unseen']:.0f}%\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"PPL                  | {metrics['PPL']:.2f}\")\n",
    "    print(f\"IQ_Pass              | {metrics['IQ_Pass']:.0f}%\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    def save_metrics_to_csv(metrics, method_name, filename=\"UGID-SEAT_Gemma.csv\"):\n",
    "        data = {\"Method\": method_name}\n",
    "        data.update(metrics)\n",
    "        df = pd.DataFrame([data])\n",
    "        ordered_columns = [\n",
    "            \"Method\",\n",
    "            \"ID_Mean\",\"ID_Max\",\n",
    "            \"OOD_Mean\",\"OOD_Max\",\n",
    "            \"Template_Mean\",\"Template_Var\",\n",
    "            \"Directional_Gap\",\"Neutral_Mass\",\n",
    "            \"Spec_Diff\",\"Hidden_Diff\",\n",
    "            \"Safety_Seen\",\"Safety_Unseen\",\n",
    "            \"PPL\",\"IQ_Pass\"\n",
    "        ]\n",
    "        final_columns = [col for col in ordered_columns if col in df.columns]\n",
    "        df = df[final_columns]\n",
    "        df.to_csv(filename, mode='a', header=not os.path.exists(filename), index=False)\n",
    "        print(f\"Results appended to: {filename}\")\n",
    "\n",
    "    save_metrics_to_csv(metrics, method_name)\n",
    "    return metrics\n",
    "\n",
    "# Run Evaluation\n",
    "run_comprehensive_evaluation(model, tokenizer, method_name=\"UGID-SEAT (Gemma-2-2B)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f8025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving UGID (Gemma-2-2B) adapters to checkpoints/Gemma2-2B/ugid ...\n",
      "✅ Gemma-2-2B UGID checkpoint saved successfully to: checkpoints/Gemma2-2B/ugid\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE UGID MODEL CHECKPOINT (Gemma-2-2B)\n",
    "# ==========================================\n",
    "import os\n",
    "\n",
    "# [修改点]: 路径改为 Gemma2-2B/ugid\n",
    "SAVE_DIR = \"checkpoints/Gemma2-2B/ugid\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving UGID (Gemma-2-2B) adapters to {SAVE_DIR} ...\")\n",
    "\n",
    "# 1. 保存 LoRA 权重\n",
    "model.save_pretrained(\n",
    "    SAVE_DIR,\n",
    "    safe_serialization=True  \n",
    ")\n",
    "\n",
    "# 2. 保存 Tokenizer\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# 3. 保存说明文件\n",
    "with open(os.path.join(SAVE_DIR, \"README.txt\"), \"w\") as f:\n",
    "    f.write(\"Model: google/gemma-2-2b\\n\")\n",
    "    f.write(\"Method: UGID (Unified Graph Isomorphism Debiasing)\\n\")\n",
    "    # [重要]: 这里记录的是训练 Gemma 时实际使用的层数\n",
    "    f.write(\"Target Layers: [10, 12, 14]\\n\") \n",
    "\n",
    "print(f\"✅ Gemma-2-2B UGID checkpoint saved successfully to: {SAVE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bias)",
   "language": "python",
   "name": "bias"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
