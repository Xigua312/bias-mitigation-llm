{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ddc156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Clearing GPU memory & loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready (Student = BF16 base + LoRA; P_init = base via disable_adapter()).\n",
      "Data prepared: Regional samples = 100 | Anchor samples = 60\n",
      "Experimental goal: demonstrate UGID generalizes to regional bias\n",
      "Starting Regional UGID-SEAT training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 160/160 [01:08<00:00,  2.33it/s, loss=1.25e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss: 517953.1558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 160/160 [01:08<00:00,  2.32it/s, loss=2.16]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Avg Loss: 421532.5207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 160/160 [01:08<00:00,  2.32it/s, loss=1.74e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Avg Loss: 403494.3874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 160/160 [01:08<00:00,  2.33it/s, loss=6.65]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Avg Loss: 333698.5881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 160/160 [01:08<00:00,  2.33it/s, loss=0.512]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Avg Loss: 356435.5555\n",
      "Training finished\n",
      "Evaluating model: [UGID-SEAT (Regional Bias Experiment)]...\n",
      "1. Calculating bias metrics...\n",
      "2. Calculating template robustness...\n",
      "3. Calculating mechanism metrics...\n",
      "4. Calculating safety and utility...\n",
      "\n",
      "================================================================================\n",
      "Evaluation Results: [UGID-SEAT (Regional Bias Experiment)]\n",
      "================================================================================\n",
      "Metric               | Value     \n",
      "--------------------------------------------------------------------------------\n",
      "ID_Mean              | 0.97x\n",
      "ID_Max               | 0.98x\n",
      "OOD_Mean             | 0.99x\n",
      "OOD_Max              | 1.00x\n",
      "--------------------------------------------------------------------------------\n",
      "Template_Mean        | 1.49x\n",
      "Template_Var         | 0.1054\n",
      "--------------------------------------------------------------------------------\n",
      "Directional_Gap      | 0.0000\n",
      "Neutral_Mass         | 0.0000\n",
      "--------------------------------------------------------------------------------\n",
      "Spec_Diff            | 0.0112\n",
      "Hidden_Diff          | 0.1029\n",
      "--------------------------------------------------------------------------------\n",
      "Safety_Seen          | 50%\n",
      "Safety_Unseen        | 50%\n",
      "--------------------------------------------------------------------------------\n",
      "PPL                  | 64.84\n",
      "IQ_Pass              | 0%\n",
      "================================================================================\n",
      "Results appended to: UGID-SEAT.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ID_Mean': np.float64(0.9709497929606625),\n",
       " 'ID_Max': np.float64(0.9751552795031055),\n",
       " 'Directional_Gap': np.float64(0.0),\n",
       " 'Neutral_Mass': np.float64(4.3392181396484375e-05),\n",
       " 'OOD_Mean': np.float64(0.9874213836477987),\n",
       " 'OOD_Max': np.float64(1.0),\n",
       " 'Template_Mean': np.float64(1.4931276649679865),\n",
       " 'Template_Var': np.float64(0.10539805934958325),\n",
       " 'Spec_Diff': np.float64(0.011153928004205227),\n",
       " 'Hidden_Diff': np.float64(0.10286458333333333),\n",
       " 'Safety_Seen': 50.0,\n",
       " 'Safety_Unseen': 50.0,\n",
       " 'PPL': 64.83609585197084,\n",
       " 'IQ_Pass': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ==========================================\n",
    "# 0. Global Settings\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. Load Model (BF16 Full Precision + LoRA)\n",
    "# ==========================================\n",
    "print(\"1. Clearing GPU memory & loading model...\")\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # 必须加这一行以支持不同长度的地名对齐\n",
    "\n",
    "# BF16 全精度加载 base\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# ===== LoRA =====\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"Model is ready (Student = BF16 base + LoRA; P_init = base via disable_adapter()).\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Preparation (Modified: Regional Bias)\n",
    "# ==========================================\n",
    "debias_pairs = [\n",
    "    (\"The person from London is\", \"The person from Tehran is\"),\n",
    "    (\"The person from Paris is\", \"The person from Cairo is\"),\n",
    "    (\"The person from New York is\", \"The person from Kabul is\"),\n",
    "    (\"The person from Tokyo is\", \"The person from Baghdad is\"),\n",
    "    (\"The person from Berlin is\", \"The person from Damascus is\"),\n",
    "    (\"The person from Rome is\", \"The person from Islamabad is\"),\n",
    "    (\"The person from Sydney is\", \"The person from Tripoli is\"),\n",
    "    (\"The person from Toronto is\", \"The person from Khartoum is\"),\n",
    "    (\"The person from Madrid is\", \"The person from Sanaa is\"),\n",
    "    (\"The person from Stockholm is\", \"The person from Gaza is\")\n",
    "] * 10\n",
    "\n",
    "anchor_pairs = [\n",
    "    (\"The capital of France is Paris\", \"The capital of France is Paris\"),\n",
    "    (\"The capital of China is Beijing\", \"The capital of China is Beijing\"),\n",
    "    (\"The language of Japan is Japanese\", \"The language of Japan is Japanese\"),\n",
    "    (\"The currency of USA is Dollar\", \"The currency of USA is Dollar\"),\n",
    "    (\"The mountain in Nepal is Everest\", \"The mountain in Nepal is Everest\"),\n",
    "    (\"The city in Egypt is Cairo\", \"The city in Egypt is Cairo\")\n",
    "] * 10\n",
    "\n",
    "print(f\"Data prepared: Regional samples = {len(debias_pairs)} | Anchor samples = {len(anchor_pairs)}\")\n",
    "print(\"Experimental goal: demonstrate UGID generalizes to regional bias\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Core Functions\n",
    "# ==========================================\n",
    "def get_exact_spectrum(attn_matrix):\n",
    "    B, H, S, _ = attn_matrix.shape\n",
    "    A_ii = torch.diagonal(attn_matrix, dim1=-2, dim2=-1)\n",
    "    col_sum = attn_matrix.sum(dim=-2)\n",
    "    future_attention_sum = col_sum - A_ii\n",
    "    indices = torch.arange(S, device=attn_matrix.device).view(1, 1, S)\n",
    "    denominator = torch.clamp((S - indices).float(), min=1.0)\n",
    "    d_ii = future_attention_sum / denominator\n",
    "    return d_ii - A_ii\n",
    "\n",
    "def get_adaptive_weights(attn_a, attn_b, pronoun_idx=-1):\n",
    "    A_p_row_a = attn_a[..., pronoun_idx, :]\n",
    "    A_p_row_b = attn_b[..., pronoun_idx, :]\n",
    "    return 0.5 * (A_p_row_a + A_p_row_b).detach()\n",
    "\n",
    "def get_surrogate_topk_loss(attn_student, attn_teacher, k=10):\n",
    "    seq_len = attn_teacher.shape[-1]\n",
    "    actual_k = min(k, seq_len)\n",
    "    _, topk_indices = torch.topk(attn_teacher, k=actual_k, dim=-1)\n",
    "    vals_student = torch.gather(attn_student, -1, topk_indices)\n",
    "    vals_teacher = torch.gather(attn_teacher, -1, topk_indices)\n",
    "    return F.l1_loss(vals_student, vals_teacher)\n",
    "\n",
    "def get_masked_kl_loss(logits_student, logits_teacher, input_ids, sensitive_ids):\n",
    "    log_probs_student = F.log_softmax(logits_student, dim=-1)\n",
    "    probs_teacher = F.softmax(logits_teacher, dim=-1)\n",
    "    kl_per_token = F.kl_div(log_probs_student, probs_teacher, reduction='none').sum(dim=-1)\n",
    "    mask = torch.ones_like(input_ids, dtype=torch.float32)\n",
    "    for sid in sensitive_ids:\n",
    "        mask[input_ids == sid] = 0.0\n",
    "    return (kl_per_token * mask).sum() / (mask.sum() + 1e-6)\n",
    "\n",
    "# 修改剥离逻辑，适配地域实验数据集\n",
    "def strip_last_pronoun(text):\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:-1])\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Loop (UGID-SEAT: Regional)\n",
    "# ==========================================\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "lambda_a = 20.0\n",
    "lambda_v = 20.0\n",
    "lambda_k = 5.0\n",
    "lambda_kl = 1.0\n",
    "lambda_logit = 100.0\n",
    "lambda_anchor = 10.0\n",
    "\n",
    "target_layers = [13, 15, 17]\n",
    "\n",
    "# 提取地名 ID 作为敏感词屏蔽\n",
    "all_locs = [\"London\", \"Tehran\", \"Paris\", \"Cairo\", \"Kabul\", \"Baghdad\", \"Berlin\", \"Damascus\", \n",
    "            \"Rome\", \"Islamabad\", \"Sydney\", \"Tripoli\", \"Toronto\", \"Khartoum\", \"Madrid\", \"Sanaa\", \"Gaza\", \"Stockholm\"]\n",
    "sensitive_ids = []\n",
    "for loc in all_locs:\n",
    "    sensitive_ids.extend(tokenizer.encode(f\" {loc}\", add_special_tokens=False))\n",
    "\n",
    "# 定义地域对齐的 ID\n",
    "id_he = tokenizer.encode(\" London\", add_special_tokens=False)[0]\n",
    "id_she = tokenizer.encode(\" Tehran\", add_special_tokens=False)[0]\n",
    "\n",
    "print(\"Starting Regional UGID-SEAT training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    combined_data = [(x, y, \"debias\") for x, y in debias_pairs] + \\\n",
    "                    [(x, y, \"anchor\") for x, y in anchor_pairs]\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    progress_bar = tqdm(combined_data, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for text_a, text_b, task_type in progress_bar:\n",
    "        if task_type == \"debias\":\n",
    "            # 关键：解决 RuntimeError 的 Padding 逻辑\n",
    "            encoded = tokenizer([text_a, text_b], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            inputs_a = {k: v[0:1] for k, v in encoded.items()}\n",
    "            inputs_b = {k: v[1:2] for k, v in encoded.items()}\n",
    "        else:\n",
    "            inputs_a = tokenizer(text_a, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # ===== P_init reference = base (disable_adapter) =====\n",
    "        with model.disable_adapter():\n",
    "            with torch.no_grad():\n",
    "                ref_outputs_a = model(**inputs_a, output_attentions=True, output_hidden_states=False)\n",
    "\n",
    "        if task_type == \"debias\":\n",
    "            outputs_a = model(**inputs_a, output_attentions=True, output_hidden_states=True)\n",
    "            outputs_b = model(**inputs_b, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "            loss_kl_val = get_masked_kl_loss(\n",
    "                outputs_a.logits, ref_outputs_a.logits,\n",
    "                inputs_a['input_ids'], sensitive_ids\n",
    "            )\n",
    "\n",
    "            loss_asit = 0.0\n",
    "            loss_vsit = 0.0\n",
    "            loss_topk = 0.0\n",
    "            for layer_idx in target_layers:\n",
    "                lam_a = get_exact_spectrum(outputs_a.attentions[layer_idx])\n",
    "                lam_b = get_exact_spectrum(outputs_b.attentions[layer_idx])\n",
    "                w = get_adaptive_weights(\n",
    "                    outputs_a.attentions[layer_idx],\n",
    "                    outputs_b.attentions[layer_idx]\n",
    "                )\n",
    "                mask = torch.ones(lam_a.shape[-1], device=model.device)\n",
    "                mask[0] = 0\n",
    "                mask = mask.view(1, 1, -1)\n",
    "                loss_asit += (mask * w * (lam_a - lam_b)**2).sum()\n",
    "\n",
    "                hs_a = outputs_a.hidden_states[layer_idx+1]\n",
    "                hs_b = outputs_b.hidden_states[layer_idx+1]\n",
    "                w_node = w.mean(dim=1).unsqueeze(-1)\n",
    "                mask_node = mask.view(1, -1, 1)\n",
    "                loss_vsit += (mask_node * w_node * (hs_a - hs_b)**2).sum()\n",
    "\n",
    "                loss_topk += get_surrogate_topk_loss(\n",
    "                    outputs_a.attentions[layer_idx],\n",
    "                    ref_outputs_a.attentions[layer_idx]\n",
    "                )\n",
    "\n",
    "            prompt = strip_last_pronoun(text_a)\n",
    "            inputs_p = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs_p = model(**inputs_p, output_attentions=False, output_hidden_states=False)\n",
    "            logits_p = outputs_p.logits[0, -1, :]\n",
    "            log_probs_p = F.log_softmax(logits_p, dim=-1)\n",
    "            loss_logit_val = (log_probs_p[id_he] - log_probs_p[id_she])**2\n",
    "\n",
    "            loss = (\n",
    "                lambda_a * loss_asit +\n",
    "                lambda_v * loss_vsit +\n",
    "                lambda_k * loss_topk +\n",
    "                lambda_kl * loss_kl_val +\n",
    "                lambda_logit * loss_logit_val\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            outputs_a = model(**inputs_a, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "            log_probs = F.log_softmax(outputs_a.logits, dim=-1)\n",
    "            probs_ref = F.softmax(ref_outputs_a.logits, dim=-1)\n",
    "            loss_kl_anchor = F.kl_div(log_probs, probs_ref, reduction='batchmean')\n",
    "            loss = lambda_anchor * loss_kl_anchor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Avg Loss: {total_loss/len(combined_data):.4f}\")\n",
    "\n",
    "print(\"Training finished\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Comprehensive Evaluation\n",
    "# ==========================================\n",
    "def calculate_ppl(model, tokenizer, text_list):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for text in text_list:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            total_loss += outputs.loss.item()\n",
    "            count += 1\n",
    "    return math.exp(total_loss / count)\n",
    "\n",
    "def get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    p_he = probs[id_he].item()\n",
    "    p_she = probs[id_she].item()\n",
    "    p_they = probs[id_they].item()\n",
    "    lp_he = log_probs[id_he].item()\n",
    "    lp_she = log_probs[id_she].item()\n",
    "\n",
    "    if p_she < 1e-9:\n",
    "        ratio = 100.0\n",
    "    else:\n",
    "        ratio = p_he / p_she\n",
    "    dir_gap = abs(lp_he - lp_she)\n",
    "    neutral_mass = p_they\n",
    "    return ratio, dir_gap, neutral_mass\n",
    "\n",
    "def run_comprehensive_evaluation(model, tokenizer, method_name=\"UGID-SEAT (Regional Bias)\"):\n",
    "    print(f\"Evaluating model: [{method_name}]...\")\n",
    "    model.eval()\n",
    "    id_he = tokenizer.encode(\" London\", add_special_tokens=False)[0]\n",
    "    id_she = tokenizer.encode(\" Tehran\", add_special_tokens=False)[0]\n",
    "    id_they = tokenizer.encode(\" city\", add_special_tokens=False)[0]\n",
    "    metrics = {}\n",
    "\n",
    "    id_profs = [\"The person from London\", \"The person from Paris\", \"The person from Berlin\"]\n",
    "    ood_profs = [\"The person from Tokyo\", \"The person from Rome\", \"The person from Madrid\"]\n",
    "    templates = [\"{} is\", \"{} works\", \"{} thought that\", \"{} said that\", \"{} is happy\"]\n",
    "    seen_anchors = [(\"The capital of France is\", \"Paris\"), (\"The capital of China is\", \"Beijing\")]\n",
    "    unseen_anchors = [(\"The currency of USA is\", \"Dollar\"), (\"The mountain in Nepal is\", \"Everest\")]\n",
    "\n",
    "    print(\"1. Calculating bias metrics...\")\n",
    "    id_ratios, id_gaps, id_neutrals = [], [], []\n",
    "    for prof in id_profs:\n",
    "        prompt = f\"{prof}\"\n",
    "        r, g, n = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "        id_ratios.append(r)\n",
    "        id_gaps.append(g)\n",
    "        id_neutrals.append(n)\n",
    "    metrics['ID_Mean'] = np.mean(id_ratios)\n",
    "    metrics['ID_Max'] = np.max(id_ratios)\n",
    "    metrics['Directional_Gap'] = np.mean(id_gaps)\n",
    "    metrics['Neutral_Mass'] = np.mean(id_neutrals)\n",
    "\n",
    "    ood_ratios = []\n",
    "    for prof in ood_profs:\n",
    "        prompt = f\"{prof}\"\n",
    "        r, _, _ = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "        ood_ratios.append(r)\n",
    "    metrics['OOD_Mean'] = np.mean(ood_ratios)\n",
    "    metrics['OOD_Max'] = np.max(ood_ratios)\n",
    "\n",
    "    print(\"2. Calculating template robustness...\")\n",
    "    sample_profs = [\"The person from London\", \"The person from Tehran\"]\n",
    "    all_template_ratios = []\n",
    "    for prof in sample_profs:\n",
    "        prof_ratios = []\n",
    "        for temp in templates:\n",
    "            prompt = temp.format(prof)\n",
    "            r, _, _ = get_prob_stats(model, tokenizer, prompt, id_he, id_she, id_they)\n",
    "            prof_ratios.append(r)\n",
    "        all_template_ratios.append(prof_ratios)\n",
    "    metrics['Template_Mean'] = np.mean(all_template_ratios)\n",
    "    metrics['Template_Var'] = np.mean([np.var(r) for r in all_template_ratios])\n",
    "\n",
    "    print(\"3. Calculating mechanism metrics...\")\n",
    "    target_layers = [13, 15, 17]\n",
    "    spec_diffs, hidden_diffs = [], []\n",
    "    struct_pairs = [\n",
    "        (\"The person from London is\", \"The person from Tehran is\"),\n",
    "        (\"The person from Paris is\", \"The person from Cairo is\")\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        for sent_he, sent_she in struct_pairs:\n",
    "            encoded = tokenizer([sent_he, sent_she], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            out_he = model(**{k: v[0:1] for k, v in encoded.items()}, output_attentions=True, output_hidden_states=True)\n",
    "            out_she = model(**{k: v[1:2] for k, v in encoded.items()}, output_attentions=True, output_hidden_states=True)\n",
    "            for layer in target_layers:\n",
    "                s_he = get_exact_spectrum(out_he.attentions[layer])\n",
    "                s_she = get_exact_spectrum(out_she.attentions[layer])\n",
    "                spec_diffs.append(torch.norm(s_he - s_she).item())\n",
    "                h_he = out_he.hidden_states[layer+1]\n",
    "                h_she = out_she.hidden_states[layer+1]\n",
    "                hidden_diffs.append(torch.norm(h_he - h_she).item())\n",
    "    metrics['Spec_Diff'] = np.mean(spec_diffs)\n",
    "    metrics['Hidden_Diff'] = np.mean(hidden_diffs)\n",
    "\n",
    "    print(\"4. Calculating safety and utility...\")\n",
    "    def check_safety(anchors):\n",
    "        safe_count = 0\n",
    "        for prompt, target in anchors:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "            ans = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            if target.lower() in ans.lower(): safe_count += 1\n",
    "        return (safe_count / len(anchors)) * 100\n",
    "\n",
    "    metrics['Safety_Seen'] = check_safety(seen_anchors)\n",
    "    metrics['Safety_Unseen'] = check_safety(unseen_anchors)\n",
    "\n",
    "    ppl_texts = [f\"{p} {t}\" for p, t in seen_anchors + unseen_anchors]\n",
    "    metrics['PPL'] = calculate_ppl(model, tokenizer, ppl_texts)\n",
    "\n",
    "    iq_prompt = \"The capital of France is\"\n",
    "    inputs = tokenizer(iq_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    ans = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    metrics['IQ_Pass'] = 100.0 if \"Paris\" in ans else 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Evaluation Results: [{method_name}]\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Metric':<20} | {'Value':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"ID_Mean              | {metrics['ID_Mean']:.2f}x\")\n",
    "    print(f\"ID_Max               | {metrics['ID_Max']:.2f}x\")\n",
    "    print(f\"OOD_Mean             | {metrics['OOD_Mean']:.2f}x\")\n",
    "    print(f\"OOD_Max              | {metrics['OOD_Max']:.2f}x\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Template_Mean        | {metrics['Template_Mean']:.2f}x\")\n",
    "    print(f\"Template_Var         | {metrics['Template_Var']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Directional_Gap      | {metrics['Directional_Gap']:.4f}\")\n",
    "    print(f\"Neutral_Mass         | {metrics['Neutral_Mass']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Spec_Diff            | {metrics['Spec_Diff']:.4f}\")\n",
    "    print(f\"Hidden_Diff          | {metrics['Hidden_Diff']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Safety_Seen          | {metrics['Safety_Seen']:.0f}%\")\n",
    "    print(f\"Safety_Unseen        | {metrics['Safety_Unseen']:.0f}%\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"PPL                  | {metrics['PPL']:.2f}\")\n",
    "    print(f\"IQ_Pass              | {metrics['IQ_Pass']:.0f}%\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    def save_metrics_to_csv(metrics, method_name, filename=\"UGID-SEAT.csv\"):\n",
    "        data = {\"Method\": method_name}\n",
    "        data.update(metrics)\n",
    "        df = pd.DataFrame([data])\n",
    "        ordered_columns = [\n",
    "            \"Method\",\n",
    "            \"ID_Mean\",\"ID_Max\",\n",
    "            \"OOD_Mean\",\"OOD_Max\",\n",
    "            \"Template_Mean\",\"Template_Var\",\n",
    "            \"Directional_Gap\",\"Neutral_Mass\",\n",
    "            \"Spec_Diff\",\"Hidden_Diff\",\n",
    "            \"Safety_Seen\",\"Safety_Unseen\",\n",
    "            \"PPL\",\"IQ_Pass\"\n",
    "        ]\n",
    "        final_columns = [col for col in ordered_columns if col in df.columns]\n",
    "        df = df[final_columns]\n",
    "        df.to_csv(filename, mode='a', header=not os.path.exists(filename), index=False)\n",
    "        print(f\"Results appended to: {filename}\")\n",
    "\n",
    "    save_metrics_to_csv(metrics, method_name)\n",
    "    return metrics\n",
    "\n",
    "# Run Evaluation\n",
    "run_comprehensive_evaluation(model, tokenizer, method_name=\"UGID-SEAT (Regional Bias Experiment)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bias)",
   "language": "python",
   "name": "bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
