{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8557f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zikang.ding/envs/bias/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Clearing GPU memory & loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready: BF16 base + LoRA(attn-only).\n",
      "Data prepared: Debias=100 | Anchor=60\n",
      "Starting training: UGID-attn-only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 160/160 [01:05<00:00,  2.45it/s, loss=0.00318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss: 0.0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 160/160 [01:04<00:00,  2.49it/s, loss=0.0442]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Avg Loss: 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 160/160 [01:04<00:00,  2.49it/s, loss=0.000546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Avg Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 160/160 [01:04<00:00,  2.49it/s, loss=0.000554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Avg Loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 160/160 [01:04<00:00,  2.49it/s, loss=0.00252] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Avg Loss: 0.0053\n",
      "Training finished: UGID-attn-only\n",
      "[Saved] LoRA adapter to: ./checkpoints/ugid_attn_only\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ==========================================\n",
    "# 0. Global Settings\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "SAVE_DIR = \"./checkpoints/ugid_attn_only\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 1. Load Model (BF16 Full Precision + LoRA)\n",
    "# ==========================================\n",
    "print(\"1) Clearing GPU memory & loading model...\")\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# ===== LoRA (ATTENTION ONLY) =====\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # << attn-only\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"Model is ready: BF16 base + LoRA(attn-only).\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data\n",
    "# ==========================================\n",
    "debias_pairs = [\n",
    "    (\"The doctor said that he\", \"The doctor said that she\"),\n",
    "    (\"The nurse said that she\", \"The nurse said that he\"),\n",
    "    (\"The engineer said that he\", \"The engineer said that she\"),\n",
    "    (\"The teacher said that he\", \"The teacher said that she\"),\n",
    "    (\"The CEO said that he\", \"The CEO said that she\"),\n",
    "    (\"The secretary said that she\", \"The secretary said that he\"),\n",
    "    (\"The developer said that he\", \"The developer said that she\"),\n",
    "    (\"The manager said that he\", \"The manager said that she\"),\n",
    "    (\"The cleaner said that she\", \"The cleaner said that he\"),\n",
    "    (\"The driver said that he\", \"The driver said that she\"),\n",
    "] * 10\n",
    "\n",
    "anchor_pairs = [\n",
    "    (\"The king said that he\", \"The king said that he\"),\n",
    "    (\"The queen said that she\", \"The queen said that she\"),\n",
    "    (\"The father said that he\", \"The father said that he\"),\n",
    "    (\"The mother said that she\", \"The mother said that she\"),\n",
    "    (\"The brother said that he\", \"The brother said that he\"),\n",
    "    (\"The sister said that she\", \"The sister said that she\"),\n",
    "] * 10\n",
    "\n",
    "print(f\"Data prepared: Debias={len(debias_pairs)} | Anchor={len(anchor_pairs)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Core Functions\n",
    "# ==========================================\n",
    "def get_exact_spectrum(attn_matrix):\n",
    "    B, H, S, _ = attn_matrix.shape\n",
    "    A_ii = torch.diagonal(attn_matrix, dim1=-2, dim2=-1)\n",
    "    col_sum = attn_matrix.sum(dim=-2)\n",
    "    future_attention_sum = col_sum - A_ii\n",
    "    indices = torch.arange(S, device=attn_matrix.device).view(1, 1, S)\n",
    "    denominator = torch.clamp((S - indices).float(), min=1.0)\n",
    "    d_ii = future_attention_sum / denominator\n",
    "    return d_ii - A_ii\n",
    "\n",
    "def get_adaptive_weights(attn_a, attn_b, pronoun_idx=-1):\n",
    "    A_p_row_a = attn_a[..., pronoun_idx, :]\n",
    "    A_p_row_b = attn_b[..., pronoun_idx, :]\n",
    "    return 0.5 * (A_p_row_a + A_p_row_b).detach()\n",
    "\n",
    "def get_surrogate_topk_loss(attn_student, attn_teacher, k=10):\n",
    "    seq_len = attn_teacher.shape[-1]\n",
    "    actual_k = min(k, seq_len)\n",
    "    _, topk_indices = torch.topk(attn_teacher, k=actual_k, dim=-1)\n",
    "    vals_student = torch.gather(attn_student, -1, topk_indices)\n",
    "    vals_teacher = torch.gather(attn_teacher, -1, topk_indices)\n",
    "    return F.l1_loss(vals_student, vals_teacher)\n",
    "\n",
    "def get_masked_kl_loss(logits_student, logits_teacher, input_ids, sensitive_ids):\n",
    "    log_probs_student = F.log_softmax(logits_student, dim=-1)\n",
    "    probs_teacher = F.softmax(logits_teacher, dim=-1)\n",
    "    kl_per_token = F.kl_div(log_probs_student, probs_teacher, reduction='none').sum(dim=-1)\n",
    "    mask = torch.ones_like(input_ids, dtype=torch.float32)\n",
    "    for sid in sensitive_ids:\n",
    "        mask[input_ids == sid] = 0.0\n",
    "    return (kl_per_token * mask).sum() / (mask.sum() + 1e-6)\n",
    "\n",
    "def strip_last_pronoun(text):\n",
    "    if text.endswith(\" he\"):\n",
    "        return text[:-3]\n",
    "    if text.endswith(\" she\"):\n",
    "        return text[:-4]\n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training (UGID-attn-only)\n",
    "# ==========================================\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ===== weights =====\n",
    "lambda_a = 20.0     # ASIT ON\n",
    "lambda_v = 0.0      # VSIT OFF\n",
    "lambda_k = 0.0\n",
    "lambda_kl = 0.0\n",
    "lambda_logit = 0.0\n",
    "lambda_anchor = 1.0  # <<<<< 降低 anchor 稳定\n",
    "\n",
    "target_layers = [13, 15, 17]\n",
    "sensitive_ids = [tokenizer.encode(\" he\")[1], tokenizer.encode(\" she\")[1]]\n",
    "id_he, id_she = sensitive_ids\n",
    "\n",
    "print(\"Starting training: UGID-attn-only...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0.0\n",
    "    combined_data = [(x, y, \"debias\") for x, y in debias_pairs] + [(x, y, \"anchor\") for x, y in anchor_pairs]\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    progress_bar = tqdm(combined_data, desc=f\"Epoch {epoch+1}\")\n",
    "    for text_a, text_b, task_type in progress_bar:\n",
    "        inputs_a = tokenizer(text_a, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # P_init reference = base (disable_adapter)\n",
    "        with model.disable_adapter():\n",
    "            with torch.no_grad():\n",
    "                ref_outputs_a = model(**inputs_a, output_attentions=True, output_hidden_states=False)\n",
    "\n",
    "        if task_type == \"debias\":\n",
    "            inputs_b = tokenizer(text_b, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            outputs_a = model(**inputs_a, output_attentions=True, output_hidden_states=True)\n",
    "            outputs_b = model(**inputs_b, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "            loss_kl_val = get_masked_kl_loss(outputs_a.logits, ref_outputs_a.logits, inputs_a.input_ids, sensitive_ids)\n",
    "\n",
    "            loss_asit = 0.0\n",
    "            loss_topk = 0.0\n",
    "\n",
    "            for layer_idx in target_layers:\n",
    "                lam_a = get_exact_spectrum(outputs_a.attentions[layer_idx])\n",
    "                lam_b = get_exact_spectrum(outputs_b.attentions[layer_idx])\n",
    "                w = get_adaptive_weights(outputs_a.attentions[layer_idx], outputs_b.attentions[layer_idx])\n",
    "\n",
    "                mask = torch.ones(lam_a.shape[-1], device=model.device)\n",
    "                mask[0] = 0\n",
    "                mask = mask.view(1, 1, -1)\n",
    "\n",
    "                loss_asit += (mask * w * (lam_a - lam_b) ** 2).sum()\n",
    "                loss_topk += get_surrogate_topk_loss(outputs_a.attentions[layer_idx], ref_outputs_a.attentions[layer_idx])\n",
    "\n",
    "            # behavior term: same as your eval prompt position\n",
    "            prompt = strip_last_pronoun(text_a)\n",
    "            inputs_p = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs_p = model(**inputs_p, output_attentions=False, output_hidden_states=False)\n",
    "            logits_p = outputs_p.logits[0, -1, :]\n",
    "            log_probs_p = F.log_softmax(logits_p, dim=-1)\n",
    "            loss_logit_val = (log_probs_p[id_he] - log_probs_p[id_she]) ** 2\n",
    "\n",
    "            loss = (\n",
    "                lambda_a * loss_asit +\n",
    "                lambda_k * loss_topk +\n",
    "                lambda_kl * loss_kl_val +\n",
    "                lambda_logit * loss_logit_val\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            outputs_a = model(**inputs_a, output_attentions=False, output_hidden_states=False)\n",
    "            log_probs = F.log_softmax(outputs_a.logits, dim=-1)\n",
    "            probs_ref = F.softmax(ref_outputs_a.logits, dim=-1)\n",
    "            loss_kl_anchor = F.kl_div(log_probs, probs_ref, reduction='batchmean')\n",
    "            loss = lambda_anchor * loss_kl_anchor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Avg Loss: {total_loss/len(combined_data):.4f}\")\n",
    "\n",
    "print(\"Training finished: UGID-attn-only\")\n",
    "\n",
    "# ==========================================\n",
    "# 5) Save LoRA adapter\n",
    "# ==========================================\n",
    "model.eval()\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(f\"[Saved] LoRA adapter to: {SAVE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bias)",
   "language": "python",
   "name": "bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
