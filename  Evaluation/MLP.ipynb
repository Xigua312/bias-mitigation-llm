{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1481615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model (UGID-MLP-only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready: BF16 base + LoRA(MLP-only)\n",
      "Data prepared: Debias=100 | Anchor=40\n",
      "Training UGID-MLP-only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 140/140 [00:34<00:00,  4.03it/s, loss=0.0068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 0.1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 140/140 [00:35<00:00,  3.90it/s, loss=0.0144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 140/140 [00:36<00:00,  3.80it/s, loss=0.0035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 140/140 [00:33<00:00,  4.20it/s, loss=0.0688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 140/140 [00:33<00:00,  4.20it/s, loss=0.0046] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 0.0343\n",
      "[Saved] UGID-MLP-only to ./checkpoints/ugid_mlp_only\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# UGID-MLP-only (FINAL, reviewer-proof)\n",
    "# Purpose:\n",
    "#   Show that constraining ONLY MLP (nodes)\n",
    "#   cannot block bias routing via attention.\n",
    "# ==========================================\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --------------------------\n",
    "# 0. Seed\n",
    "# --------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "SAVE_DIR = \"./checkpoints/ugid_mlp_only\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load base model (MLP-only LoRA)\n",
    "# --------------------------\n",
    "print(\"Loading base model (UGID-MLP-only)...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=False,      # ★ 不需要 attention loss\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "# ===== LoRA: MLP ONLY =====\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"],  # ★ MLP-only\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.train()\n",
    "\n",
    "print(\"Model ready: BF16 base + LoRA(MLP-only)\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Data\n",
    "# --------------------------\n",
    "debias_pairs = [\n",
    "    (\"The doctor said that he\", \"The doctor said that she\"),\n",
    "    (\"The nurse said that she\", \"The nurse said that he\"),\n",
    "    (\"The engineer said that he\", \"The engineer said that she\"),\n",
    "    (\"The teacher said that he\", \"The teacher said that she\"),\n",
    "    (\"The CEO said that he\", \"The CEO said that she\"),\n",
    "    (\"The secretary said that she\", \"The secretary said that he\"),\n",
    "    (\"The developer said that he\", \"The developer said that she\"),\n",
    "    (\"The manager said that he\", \"The manager said that she\"),\n",
    "    (\"The cleaner said that she\", \"The cleaner said that he\"),\n",
    "    (\"The driver said that he\", \"The driver said that she\"),\n",
    "] * 10\n",
    "\n",
    "anchor_pairs = [\n",
    "    (\"The king said that he\", \"The king said that he\"),\n",
    "    (\"The queen said that she\", \"The queen said that she\"),\n",
    "    (\"The father said that he\", \"The father said that he\"),\n",
    "    (\"The mother said that she\", \"The mother said that she\"),\n",
    "] * 10\n",
    "\n",
    "print(f\"Data prepared: Debias={len(debias_pairs)} | Anchor={len(anchor_pairs)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 3. Loss utilities\n",
    "# --------------------------\n",
    "def get_masked_kl_loss(logits_student, logits_teacher, input_ids, sensitive_ids):\n",
    "    \"\"\"\n",
    "    Stability term: keep non-sensitive tokens close to base model.\n",
    "    \"\"\"\n",
    "    logp_s = F.log_softmax(logits_student, dim=-1)\n",
    "    p_t = F.softmax(logits_teacher, dim=-1)\n",
    "    kl = F.kl_div(logp_s, p_t, reduction=\"none\").sum(dim=-1)\n",
    "\n",
    "    mask = torch.ones_like(input_ids, dtype=torch.float32)\n",
    "    for sid in sensitive_ids:\n",
    "        mask[input_ids == sid] = 0.0\n",
    "\n",
    "    return (kl * mask).sum() / (mask.sum() + 1e-6)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Training setup\n",
    "# --------------------------\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ===== Loss weights =====\n",
    "lambda_v = 20.0      # ★ ONLY node / MLP constraint\n",
    "lambda_kl = 1.0      # ★ stability, but weak\n",
    "lambda_anchor = 1.0  # ★★★ CRITICAL: small anchor, do NOT hide failure\n",
    "lambda_logit = 0.0   # ★★★ MUST be zero (no behavioral shortcut)\n",
    "\n",
    "target_layers = [13, 15, 17]\n",
    "sensitive_ids = [tokenizer.encode(\" he\")[1], tokenizer.encode(\" she\")[1]]\n",
    "\n",
    "print(\"Training UGID-MLP-only...\")\n",
    "\n",
    "# --------------------------\n",
    "# 5. Training loop\n",
    "# --------------------------\n",
    "for epoch in range(5):\n",
    "    data = [(a, b, \"debias\") for a, b in debias_pairs] + \\\n",
    "           [(a, b, \"anchor\") for a, b in anchor_pairs]\n",
    "    random.shuffle(data)\n",
    "\n",
    "    pbar = tqdm(data, desc=f\"Epoch {epoch+1}\")\n",
    "    total = 0.0\n",
    "\n",
    "    for a, b, typ in pbar:\n",
    "        inp_a = tokenizer(a, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # base reference\n",
    "        with model.disable_adapter():\n",
    "            with torch.no_grad():\n",
    "                ref = model(**inp_a)\n",
    "\n",
    "        if typ == \"debias\":\n",
    "            inp_b = tokenizer(b, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            out_a = model(**inp_a, output_hidden_states=True)\n",
    "            out_b = model(**inp_b, output_hidden_states=True)\n",
    "\n",
    "            # stability\n",
    "            loss_kl = get_masked_kl_loss(\n",
    "                out_a.logits, ref.logits, inp_a.input_ids, sensitive_ids\n",
    "            )\n",
    "\n",
    "            # ===== VSIT: MLP-only, downstream-only =====\n",
    "            loss_v = 0.0\n",
    "            for l in target_layers:\n",
    "                ha = out_a.hidden_states[l+1] - out_a.hidden_states[l]\n",
    "                hb = out_b.hidden_states[l+1] - out_b.hidden_states[l]\n",
    "\n",
    "                S = ha.shape[1]\n",
    "                mask = torch.ones(S, device=ha.device)\n",
    "                mask[0] = 0        # BOS\n",
    "                mask[-1] = 0       # pronoun token (DO NOT erase bias at source)\n",
    "                mask = mask.view(1, -1, 1)\n",
    "\n",
    "                loss_v += (mask * (ha - hb) ** 2).mean()\n",
    "                \n",
    "\n",
    "            loss = lambda_v * loss_v + lambda_kl * loss_kl\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # anchor stability\n",
    "            out = model(**inp_a)\n",
    "            loss = lambda_anchor * F.kl_div(\n",
    "                F.log_softmax(out.logits, dim=-1),\n",
    "                F.softmax(ref.logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} avg loss: {total/len(data):.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 6. Save adapter\n",
    "# --------------------------\n",
    "model.eval()\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(f\"[Saved] UGID-MLP-only to {SAVE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bias)",
   "language": "python",
   "name": "bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
